# Sync gateway pod
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-geonode
  namespace: {{ .Release.Namespace }}
spec:
  # This pod can NOT be scaled, because it persists local files
  replicas: 1
  strategy:
    # Need to kill previous pods before running new ones to avoid double-attaching volumes...
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: geonode
      app.kubernetes.io/instance: {{ .Release.Name }}

  template:
    metadata:
      annotations:
        # Add a checksum to force the re-creation of the pods on every config update
        checksum/conf: {{ .Values.geonode.extraConf | sha256sum }}
        checksum/conf2: {{ .Values.geonode.extraConf2 | sha256sum }}
        checksum/nginx: {{ include "nginx_conf" . | sha256sum }}
      labels:
        app.kubernetes.io/name: geonode
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      # Resolving some hardcoded links...
      hostAliases:
      - ip: "127.0.0.1"
        hostnames:
        - geonode
        - django
      securityContext:
        fsGroup: {{ .Values.general.gid }}
      initContainers:
      - name: busybox
        image: registry.uninett.no/public/busybox:1.33.1-glibc
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: {{ .Values.general.uid }}
          runAsGroup: {{ .Values.general.gid }}
        args:
          - /bin/sh
          - -c
          - -x
          - "mkdir -p /geonode/statics && \
            mkdir -p  /geonode/backup-restore && \
            mkdir -p  /geonode/data && \
            chmod g+wx /geonode/statics && \
            chmod g+wx /geonode/backup-restore && \
            chmod g+wx /geonode/data"
        volumeMounts:
        - name: persistence
          mountPath: /geonode
          subPath: geonode
      # Wait for Postgres and rabbit
      - name: wait-db
        image: jwilder/dockerize
        imagePullPolicy: IfNotPresent
        args:
        - -timeout=120s
        - -wait
        - tcp://{{ include "database_host" .}}
        - -wait
        - tcp://{{ include "rabbit_host" .}}
      # All containers need to be in the same pod since they share volumes!
      # This also means they probably can't be scaled up... (?)
      containers:
      # This is the django app server
      - name: geonode
        image: "{{ .Values.geonode.image.name }}:{{ .Values.geonode.image.tag }}"
        env:
        {{ include "env_general" . | nindent 8 }}
        - name: IS_CELERY
          value: 'False'
        command:
        - bash
        - -c
        - |
          # Disable file logging if any
          sed -i 's/^logto/#logto/ig' /usr/src/geonode/uwsgi.ini
          # For vanilla geonode disable geoserver oauth config, as it is configured in geoserver
          sed -i "/{oauth_config}/c\        f'echo NOT MODIFYING GEOSERVER'," /usr/src/geonode/tasks.py
          # Run
          /usr/src/geonode/entrypoint.sh uwsgi --ini /usr/src/geonode/uwsgi.ini
        ports:
        - containerPort: 8000
        volumeMounts:
        - name: geonode-conf
          mountPath: /usr/src/geonode/geonode/local_settings.py
          subPath: local_settings.py
        - name: persistence
          mountPath: /mnt/volumes/statics
          subPath: geonode/statics
        - name: persistence
          mountPath: /backup_restore
          subPath: geonode/backup-restore
        - name: persistence
          mountPath: /data
          subPath: geonode/data
        - name: cache-volume
          mountPath: /tmp
        resources:
          requests:
            memory: 2Gi
            cpu: 100m
          limits:
            memory: 2Gi
            cpu: 1000m
        # This one is HORRIBLY SLOW!!!
        # Disabling readiness probe...
        # readinessProbe:
        #   tcpSocket:
        #     port: 8000
        #   initialDelaySeconds: 60
        #   periodSeconds: 10
        #   failureThreshold: 15
        livenessProbe:
          tcpSocket:
            port: 8000
          initialDelaySeconds: 1200
          periodSeconds: 10
          failureThreshold: 15

      # # Celery is the task worker
      - name: celery
        image: "{{ .Values.geonode.image.name }}:{{ .Values.geonode.image.tag }}"
        env:
        {{ include "env_general" . | nindent 8 }}
        - name: IS_CELERY
          value: 'True'
        - name: CELERY__LOG_FILE
          value: /dev/stdout
        command:
        - bash
        - -c
        - |
          # Run celery
          /usr/src/geonode/entrypoint.sh celery-cmd
        volumeMounts:
        - name: geonode-conf
          mountPath: /usr/src/geonode/geonode/local_settings.py
          subPath: local_settings.py
        # To persist uploaded media
        - name: persistence
          mountPath: /mnt/volumes/statics
          subPath: geonode/statics
        - name: persistence
          mountPath: /backup_restore
          subPath: geonode/backup-restore
        - name: persistence
          mountPath: /data
          subPath: geonode/data
        - name: cache-volume
          mountPath: /tmp
        # healthcheck: curl --fail --silent --write-out 'HTTP CODE : %{http_code}\n' --output /dev/null http://127.0.0.1:8001/
        resources:
          requests:
            memory: 1Gi
            cpu: 100m
          limits:
            memory: 2Gi
            cpu: 400m
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
        volumeMounts:
        - name: persistence
          mountPath: /mnt/volumes/statics
          subPath: geonode/statics
        - name: geonode-conf
          mountPath: /mnt/volumes/statics/static/geonode/img/favicon.ico
          subPath: favicon.ico
        - name: nginx-confd
          mountPath: /etc/nginx/conf.d
        resources:
          requests:
            memory: 128Mi
            cpu: 100m
          limits:
            memory: 256Mi
            cpu: 200m
      volumes:
      - name: nginx-confd
        configMap:
          name: {{ .Release.Name }}-nginx-confd
      - name: geonode-conf
        configMap:
          name: {{ .Release.Name }}-geonode-conf
      - name: persistence
        persistentVolumeClaim:
          claimName: {{ .Values.storage.claimName }}
      # Using an emptyDir to cache compiled statics... it will survive container crashes, but not pod restarts
      - name: cache-volume
        emptyDir: {}
